{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primera estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', input_shape=(8, 8, 12)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(NUM_LEGAL_MOVES, activation='softmax')\n",
    "])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping, reduce_lr]  \n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Accuracy (Train)</th>\n",
       "      <th>Loss (Train)</th>\n",
       "      <th>Top-5 Accuracy (Train)</th>\n",
       "      <th>Accuracy (Val)</th>\n",
       "      <th>Loss (Val)</th>\n",
       "      <th>Top-5 Accuracy (Val)</th>\n",
       "      <th>Learning Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.30%</td>\n",
       "      <td>3.4734</td>\n",
       "      <td>27.77%</td>\n",
       "      <td>8.00%</td>\n",
       "      <td>3.5972</td>\n",
       "      <td>27.14%</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8.99%</td>\n",
       "      <td>3.3815</td>\n",
       "      <td>30.72%</td>\n",
       "      <td>9.27%</td>\n",
       "      <td>5.2590</td>\n",
       "      <td>31.54%</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10.46%</td>\n",
       "      <td>3.3308</td>\n",
       "      <td>32.68%</td>\n",
       "      <td>5.05%</td>\n",
       "      <td>34.1044</td>\n",
       "      <td>24.72%</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>11.59%</td>\n",
       "      <td>3.2812</td>\n",
       "      <td>35.10%</td>\n",
       "      <td>10.26%</td>\n",
       "      <td>4.0004</td>\n",
       "      <td>32.97%</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>13.59%</td>\n",
       "      <td>3.1966</td>\n",
       "      <td>38.77%</td>\n",
       "      <td>11.06%</td>\n",
       "      <td>5.2504</td>\n",
       "      <td>33.54%</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>15.13%</td>\n",
       "      <td>3.1253</td>\n",
       "      <td>41.86%</td>\n",
       "      <td>11.95%</td>\n",
       "      <td>4.4445</td>\n",
       "      <td>33.23%</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch Accuracy (Train)  Loss (Train) Top-5 Accuracy (Train) Accuracy (Val)  \\\n",
       "0      1            7.30%        3.4734                 27.77%          8.00%   \n",
       "1      2            8.99%        3.3815                 30.72%          9.27%   \n",
       "2      3           10.46%        3.3308                 32.68%          5.05%   \n",
       "3      4           11.59%        3.2812                 35.10%         10.26%   \n",
       "4      5           13.59%        3.1966                 38.77%         11.06%   \n",
       "5      6           15.13%        3.1253                 41.86%         11.95%   \n",
       "\n",
       "   Loss (Val) Top-5 Accuracy (Val)  Learning Rate  \n",
       "0      3.5972               27.14%         0.0010  \n",
       "1      5.2590               31.54%         0.0010  \n",
       "2     34.1044               24.72%         0.0010  \n",
       "3      4.0004               32.97%         0.0010  \n",
       "4      5.2504               33.54%         0.0005  \n",
       "5      4.4445               33.23%         0.0005  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m1= pd.read_csv(\"C:/Users/josej/Downloads/training_results_modelo1_1.csv\")\n",
    "df_m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', input_shape=(8, 8, 12)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(NUM_LEGAL_MOVES, activation='softmax')\n",
    "])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.00003)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping, reduce_lr]  \n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Accuracy (Train)</th>\n",
       "      <th>Loss (Train)</th>\n",
       "      <th>Top-5 Accuracy (Train)</th>\n",
       "      <th>Accuracy (Val)</th>\n",
       "      <th>Loss (Val)</th>\n",
       "      <th>Top-5 Accuracy (Val)</th>\n",
       "      <th>Learning Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.65%</td>\n",
       "      <td>3.6583</td>\n",
       "      <td>25.79%</td>\n",
       "      <td>7.51%</td>\n",
       "      <td>3.4726</td>\n",
       "      <td>28.73%</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7.26%</td>\n",
       "      <td>3.4649</td>\n",
       "      <td>27.80%</td>\n",
       "      <td>7.74%</td>\n",
       "      <td>3.9030</td>\n",
       "      <td>28.38%</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7.35%</td>\n",
       "      <td>3.4652</td>\n",
       "      <td>27.55%</td>\n",
       "      <td>7.14%</td>\n",
       "      <td>3.5112</td>\n",
       "      <td>26.75%</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7.66%</td>\n",
       "      <td>3.4349</td>\n",
       "      <td>28.64%</td>\n",
       "      <td>7.15%</td>\n",
       "      <td>3.4933</td>\n",
       "      <td>28.75%</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>8.13%</td>\n",
       "      <td>3.4119</td>\n",
       "      <td>29.61%</td>\n",
       "      <td>7.04%</td>\n",
       "      <td>10.6193</td>\n",
       "      <td>29.57%</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>8.27%</td>\n",
       "      <td>3.4054</td>\n",
       "      <td>29.88%</td>\n",
       "      <td>6.97%</td>\n",
       "      <td>8.0397</td>\n",
       "      <td>26.02%</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch Accuracy (Train)  Loss (Train) Top-5 Accuracy (Train) Accuracy (Val)  \\\n",
       "0      1            6.65%        3.6583                 25.79%          7.51%   \n",
       "1      2            7.26%        3.4649                 27.80%          7.74%   \n",
       "2      3            7.35%        3.4652                 27.55%          7.14%   \n",
       "3      4            7.66%        3.4349                 28.64%          7.15%   \n",
       "4      5            8.13%        3.4119                 29.61%          7.04%   \n",
       "5      6            8.27%        3.4054                 29.88%          6.97%   \n",
       "\n",
       "   Loss (Val) Top-5 Accuracy (Val)  Learning Rate  \n",
       "0      3.4726               28.73%         0.0010  \n",
       "1      3.9030               28.38%         0.0010  \n",
       "2      3.5112               26.75%         0.0010  \n",
       "3      3.4933               28.75%         0.0010  \n",
       "4     10.6193               29.57%         0.0005  \n",
       "5      8.0397               26.02%         0.0005  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m1_2= pd.read_csv(\"C:/Users/josej/Downloads/training_results_modelo1_2.csv\")\n",
    "df_m1_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segunda Estructura "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', input_shape=(8, 8, 12)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),  # Aumentamos Dropout\n",
    "    tf.keras.layers.Dense(256, activation='relu'),  # Agregamos otra capa\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(NUM_LEGAL_MOVES, activation='softmax')\n",
    "])\n",
    "\n",
    "def top_5_accuracy(y_true, y_pred):\n",
    "    return tf.keras.metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=5)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\", top_5_accuracy])\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Accuracy (Train)</th>\n",
       "      <th>Loss (Train)</th>\n",
       "      <th>Top-5 Accuracy (Train)</th>\n",
       "      <th>Accuracy (Val)</th>\n",
       "      <th>Loss (Val)</th>\n",
       "      <th>Top-5 Accuracy (Val)</th>\n",
       "      <th>Learning Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.74%</td>\n",
       "      <td>3.9386</td>\n",
       "      <td>25.79%</td>\n",
       "      <td>8.48%</td>\n",
       "      <td>3.4333</td>\n",
       "      <td>29.31%</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7.65%</td>\n",
       "      <td>3.4403</td>\n",
       "      <td>28.39%</td>\n",
       "      <td>7.93%</td>\n",
       "      <td>3.3989</td>\n",
       "      <td>29.42%</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8.08%</td>\n",
       "      <td>3.4056</td>\n",
       "      <td>29.39%</td>\n",
       "      <td>9.25%</td>\n",
       "      <td>3.3735</td>\n",
       "      <td>31.28%</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8.70%</td>\n",
       "      <td>3.3792</td>\n",
       "      <td>30.42%</td>\n",
       "      <td>9.04%</td>\n",
       "      <td>3.3563</td>\n",
       "      <td>31.00%</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>9.16%</td>\n",
       "      <td>3.3611</td>\n",
       "      <td>30.83%</td>\n",
       "      <td>8.73%</td>\n",
       "      <td>3.4073</td>\n",
       "      <td>30.26%</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>9.78%</td>\n",
       "      <td>3.3420</td>\n",
       "      <td>31.71%</td>\n",
       "      <td>9.93%</td>\n",
       "      <td>3.4085</td>\n",
       "      <td>31.99%</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>10.15%</td>\n",
       "      <td>3.3198</td>\n",
       "      <td>32.22%</td>\n",
       "      <td>11.28%</td>\n",
       "      <td>3.2775</td>\n",
       "      <td>33.29%</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>10.87%</td>\n",
       "      <td>3.2965</td>\n",
       "      <td>33.26%</td>\n",
       "      <td>11.07%</td>\n",
       "      <td>3.2691</td>\n",
       "      <td>34.04%</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>11.58%</td>\n",
       "      <td>3.2700</td>\n",
       "      <td>34.47%</td>\n",
       "      <td>12.01%</td>\n",
       "      <td>3.2575</td>\n",
       "      <td>34.69%</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>12.50%</td>\n",
       "      <td>3.2307</td>\n",
       "      <td>35.92%</td>\n",
       "      <td>12.35%</td>\n",
       "      <td>3.2299</td>\n",
       "      <td>35.07%</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>13.68%</td>\n",
       "      <td>3.1803</td>\n",
       "      <td>38.08%</td>\n",
       "      <td>11.03%</td>\n",
       "      <td>3.3881</td>\n",
       "      <td>33.33%</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>15.00%</td>\n",
       "      <td>3.1284</td>\n",
       "      <td>40.58%</td>\n",
       "      <td>13.26%</td>\n",
       "      <td>3.2054</td>\n",
       "      <td>35.72%</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>16.72%</td>\n",
       "      <td>3.0518</td>\n",
       "      <td>43.85%</td>\n",
       "      <td>13.89%</td>\n",
       "      <td>3.2209</td>\n",
       "      <td>35.79%</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>18.87%</td>\n",
       "      <td>2.9595</td>\n",
       "      <td>47.79%</td>\n",
       "      <td>13.95%</td>\n",
       "      <td>3.2744</td>\n",
       "      <td>36.02%</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>20.88%</td>\n",
       "      <td>2.8658</td>\n",
       "      <td>51.32%</td>\n",
       "      <td>14.51%</td>\n",
       "      <td>3.2755</td>\n",
       "      <td>36.64%</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>24.84%</td>\n",
       "      <td>2.7075</td>\n",
       "      <td>57.45%</td>\n",
       "      <td>14.84%</td>\n",
       "      <td>3.3421</td>\n",
       "      <td>36.51%</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>27.70%</td>\n",
       "      <td>2.5862</td>\n",
       "      <td>61.48%</td>\n",
       "      <td>15.00%</td>\n",
       "      <td>3.3739</td>\n",
       "      <td>36.56%</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>29.58%</td>\n",
       "      <td>2.4937</td>\n",
       "      <td>64.29%</td>\n",
       "      <td>15.19%</td>\n",
       "      <td>3.4337</td>\n",
       "      <td>36.57%</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>32.81%</td>\n",
       "      <td>2.3666</td>\n",
       "      <td>68.19%</td>\n",
       "      <td>15.30%</td>\n",
       "      <td>3.5911</td>\n",
       "      <td>36.39%</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>34.53%</td>\n",
       "      <td>2.2904</td>\n",
       "      <td>70.10%</td>\n",
       "      <td>15.30%</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>36.08%</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>36.20%</td>\n",
       "      <td>2.2230</td>\n",
       "      <td>72.04%</td>\n",
       "      <td>15.52%</td>\n",
       "      <td>3.6506</td>\n",
       "      <td>36.39%</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>37.70%</td>\n",
       "      <td>2.1557</td>\n",
       "      <td>73.84%</td>\n",
       "      <td>15.63%</td>\n",
       "      <td>3.6804</td>\n",
       "      <td>36.53%</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch Accuracy (Train)  Loss (Train) Top-5 Accuracy (Train)  \\\n",
       "0       1            6.74%        3.9386                 25.79%   \n",
       "1       2            7.65%        3.4403                 28.39%   \n",
       "2       3            8.08%        3.4056                 29.39%   \n",
       "3       4            8.70%        3.3792                 30.42%   \n",
       "4       5            9.16%        3.3611                 30.83%   \n",
       "5       6            9.78%        3.3420                 31.71%   \n",
       "6       7           10.15%        3.3198                 32.22%   \n",
       "7       8           10.87%        3.2965                 33.26%   \n",
       "8       9           11.58%        3.2700                 34.47%   \n",
       "9      10           12.50%        3.2307                 35.92%   \n",
       "10     11           13.68%        3.1803                 38.08%   \n",
       "11     12           15.00%        3.1284                 40.58%   \n",
       "12     13           16.72%        3.0518                 43.85%   \n",
       "13     14           18.87%        2.9595                 47.79%   \n",
       "14     15           20.88%        2.8658                 51.32%   \n",
       "15     16           24.84%        2.7075                 57.45%   \n",
       "16     17           27.70%        2.5862                 61.48%   \n",
       "17     18           29.58%        2.4937                 64.29%   \n",
       "18     19           32.81%        2.3666                 68.19%   \n",
       "19     20           34.53%        2.2904                 70.10%   \n",
       "20     21           36.20%        2.2230                 72.04%   \n",
       "21     22           37.70%        2.1557                 73.84%   \n",
       "\n",
       "   Accuracy (Val)  Loss (Val) Top-5 Accuracy (Val)  Learning Rate  \n",
       "0           8.48%      3.4333               29.31%       0.000300  \n",
       "1           7.93%      3.3989               29.42%       0.000300  \n",
       "2           9.25%      3.3735               31.28%       0.000300  \n",
       "3           9.04%      3.3563               31.00%       0.000300  \n",
       "4           8.73%      3.4073               30.26%       0.000300  \n",
       "5           9.93%      3.4085               31.99%       0.000300  \n",
       "6          11.28%      3.2775               33.29%       0.000300  \n",
       "7          11.07%      3.2691               34.04%       0.000300  \n",
       "8          12.01%      3.2575               34.69%       0.000300  \n",
       "9          12.35%      3.2299               35.07%       0.000300  \n",
       "10         11.03%      3.3881               33.33%       0.000300  \n",
       "11         13.26%      3.2054               35.72%       0.000300  \n",
       "12         13.89%      3.2209               35.79%       0.000300  \n",
       "13         13.95%      3.2744               36.02%       0.000300  \n",
       "14         14.51%      3.2755               36.64%       0.000300  \n",
       "15         14.84%      3.3421               36.51%       0.000150  \n",
       "16         15.00%      3.3739               36.56%       0.000150  \n",
       "17         15.19%      3.4337               36.57%       0.000150  \n",
       "18         15.30%      3.5911               36.39%       0.000075  \n",
       "19         15.30%      3.6159               36.08%       0.000075  \n",
       "20         15.52%      3.6506               36.39%       0.000075  \n",
       "21         15.63%      3.6804               36.53%       0.000037  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m2_1= pd.read_csv(\"C:/Users/josej/Downloads/training_results_modelo2_1.csv\")\n",
    "df_m2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tercera Estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" inputs = tf.keras.Input(shape=(8, 8, 12))\n",
    "# Primera capa de convolución\n",
    "x = tf.keras.layers.Conv2D(256, (3, 3), padding='same', activation='relu')(inputs)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "# Definición de un bloque residual\n",
    "def res_block(x, filters=256, kernel_size=3):\n",
    "    shortcut = x\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size, padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size, padding='same', activation=None)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Add()([shortcut, x])\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "x = res_block(x)\n",
    "x = res_block(x)\n",
    "x = res_block(x)\n",
    "\n",
    "# Global Average Pooling para reducir la dimensión espacial\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name=\"top5_accuracy\")\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "########################################\n",
    "# 7. Entrenamiento del modelo\n",
    "########################################\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001, verbose=1)\n",
    "    ]\n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Accuracy (Train)</th>\n",
       "      <th>Loss (Train)</th>\n",
       "      <th>Top-5 Accuracy (Train)</th>\n",
       "      <th>Accuracy (Val)</th>\n",
       "      <th>Loss (Val)</th>\n",
       "      <th>Top-5 Accuracy (Val)</th>\n",
       "      <th>Learning Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.12%</td>\n",
       "      <td>6.8138</td>\n",
       "      <td>4.89%</td>\n",
       "      <td>1.84%</td>\n",
       "      <td>6.5004</td>\n",
       "      <td>6.59%</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.84%</td>\n",
       "      <td>6.5054</td>\n",
       "      <td>6.69%</td>\n",
       "      <td>2.68%</td>\n",
       "      <td>6.4699</td>\n",
       "      <td>7.93%</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2.53%</td>\n",
       "      <td>6.4474</td>\n",
       "      <td>7.65%</td>\n",
       "      <td>3.53%</td>\n",
       "      <td>6.4110</td>\n",
       "      <td>8.62%</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3.03%</td>\n",
       "      <td>6.3942</td>\n",
       "      <td>8.47%</td>\n",
       "      <td>3.87%</td>\n",
       "      <td>6.3972</td>\n",
       "      <td>9.14%</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3.35%</td>\n",
       "      <td>6.3463</td>\n",
       "      <td>8.99%</td>\n",
       "      <td>4.00%</td>\n",
       "      <td>6.3553</td>\n",
       "      <td>9.64%</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>3.62%</td>\n",
       "      <td>6.3183</td>\n",
       "      <td>9.40%</td>\n",
       "      <td>4.11%</td>\n",
       "      <td>6.3412</td>\n",
       "      <td>9.93%</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>3.83%</td>\n",
       "      <td>6.2822</td>\n",
       "      <td>9.68%</td>\n",
       "      <td>4.19%</td>\n",
       "      <td>6.3361</td>\n",
       "      <td>9.92%</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>4.26%</td>\n",
       "      <td>6.2346</td>\n",
       "      <td>10.24%</td>\n",
       "      <td>4.64%</td>\n",
       "      <td>6.2997</td>\n",
       "      <td>10.67%</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>6.1960</td>\n",
       "      <td>10.67%</td>\n",
       "      <td>5.10%</td>\n",
       "      <td>6.3518</td>\n",
       "      <td>11.08%</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>4.81%</td>\n",
       "      <td>6.1582</td>\n",
       "      <td>11.13%</td>\n",
       "      <td>5.44%</td>\n",
       "      <td>6.3146</td>\n",
       "      <td>11.51%</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>5.17%</td>\n",
       "      <td>6.1233</td>\n",
       "      <td>11.56%</td>\n",
       "      <td>5.98%</td>\n",
       "      <td>6.3914</td>\n",
       "      <td>11.99%</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch Accuracy (Train)  Loss (Train) Top-5 Accuracy (Train)  \\\n",
       "0       1            1.12%        6.8138                  4.89%   \n",
       "1       2            1.84%        6.5054                  6.69%   \n",
       "2       3            2.53%        6.4474                  7.65%   \n",
       "3       4            3.03%        6.3942                  8.47%   \n",
       "4       5            3.35%        6.3463                  8.99%   \n",
       "5       6            3.62%        6.3183                  9.40%   \n",
       "6       7            3.83%        6.2822                  9.68%   \n",
       "7       8            4.26%        6.2346                 10.24%   \n",
       "8       9            4.56%        6.1960                 10.67%   \n",
       "9      10            4.81%        6.1582                 11.13%   \n",
       "10     11            5.17%        6.1233                 11.56%   \n",
       "\n",
       "   Accuracy (Val)  Loss (Val) Top-5 Accuracy (Val)  Learning Rate  \n",
       "0           1.84%      6.5004                6.59%         0.0005  \n",
       "1           2.68%      6.4699                7.93%         0.0005  \n",
       "2           3.53%      6.4110                8.62%         0.0005  \n",
       "3           3.87%      6.3972                9.14%         0.0005  \n",
       "4           4.00%      6.3553                9.64%         0.0005  \n",
       "5           4.11%      6.3412                9.93%         0.0005  \n",
       "6           4.19%      6.3361                9.92%         0.0005  \n",
       "7           4.64%      6.2997               10.67%         0.0005  \n",
       "8           5.10%      6.3518               11.08%         0.0005  \n",
       "9           5.44%      6.3146               11.51%         0.0005  \n",
       "10          5.98%      6.3914               11.99%         0.0005  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m3_1= pd.read_csv(\"C:/Users/josej/Downloads/training_results_modelo3_1.csv\")\n",
    "df_m3_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" inputs = tf.keras.Input(shape=(8, 8, 12))\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(inputs)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "# Bloque residual sencillo\n",
    "def res_block(x, filters, kernel_size=3):\n",
    "    shortcut = x\n",
    "    y = tf.keras.layers.Conv2D(filters, kernel_size, activation='relu', padding='same')(x)\n",
    "    y = tf.keras.layers.BatchNormalization()(y)\n",
    "    y = tf.keras.layers.Conv2D(filters, kernel_size, activation=None, padding='same')(y)\n",
    "    y = tf.keras.layers.BatchNormalization()(y)\n",
    "    y = tf.keras.layers.Add()([shortcut, y])\n",
    "    y = tf.keras.layers.Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "# Bloque residual para no sobrecomplicar el modelo\n",
    "x = res_block(x, 128)\n",
    "\n",
    "# Otra capa convolucional para extraer más características\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "def sparse_categorical_focal_loss(gamma=2.0, alpha=0.25, num_classes=None):\n",
    "loss_fn = sparse_categorical_focal_loss(gamma=2.0, alpha=0.25, num_classes=num_classes)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name=\"top5_accuracy\")\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,  \n",
    "    batch_size=8,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001, verbose=1)\n",
    "    ]\n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Accuracy (Train)</th>\n",
       "      <th>Loss (Train)</th>\n",
       "      <th>Top-5 Accuracy (Train)</th>\n",
       "      <th>Accuracy (Val)</th>\n",
       "      <th>Loss (Val)</th>\n",
       "      <th>Top-5 Accuracy (Val)</th>\n",
       "      <th>Learning Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.14%</td>\n",
       "      <td>7.0250</td>\n",
       "      <td>4.61%</td>\n",
       "      <td>1.58%</td>\n",
       "      <td>6.8749</td>\n",
       "      <td>6.06%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.94%</td>\n",
       "      <td>6.5656</td>\n",
       "      <td>7.16%</td>\n",
       "      <td>3.67%</td>\n",
       "      <td>6.3865</td>\n",
       "      <td>10.37%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3.71%</td>\n",
       "      <td>6.3454</td>\n",
       "      <td>10.48%</td>\n",
       "      <td>5.68%</td>\n",
       "      <td>6.0843</td>\n",
       "      <td>14.17%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5.42%</td>\n",
       "      <td>6.1176</td>\n",
       "      <td>13.90%</td>\n",
       "      <td>8.07%</td>\n",
       "      <td>5.8341</td>\n",
       "      <td>18.83%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7.28%</td>\n",
       "      <td>5.8652</td>\n",
       "      <td>18.11%</td>\n",
       "      <td>9.35%</td>\n",
       "      <td>5.6321</td>\n",
       "      <td>22.19%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>8.65%</td>\n",
       "      <td>5.6972</td>\n",
       "      <td>21.04%</td>\n",
       "      <td>10.29%</td>\n",
       "      <td>5.5249</td>\n",
       "      <td>23.98%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>9.53%</td>\n",
       "      <td>5.5507</td>\n",
       "      <td>23.32%</td>\n",
       "      <td>10.78%</td>\n",
       "      <td>5.3468</td>\n",
       "      <td>25.55%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>10.38%</td>\n",
       "      <td>5.4444</td>\n",
       "      <td>24.88%</td>\n",
       "      <td>11.08%</td>\n",
       "      <td>5.2757</td>\n",
       "      <td>26.82%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>10.69%</td>\n",
       "      <td>5.3726</td>\n",
       "      <td>25.85%</td>\n",
       "      <td>12.04%</td>\n",
       "      <td>5.2389</td>\n",
       "      <td>28.29%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>11.04%</td>\n",
       "      <td>5.3096</td>\n",
       "      <td>26.82%</td>\n",
       "      <td>12.23%</td>\n",
       "      <td>5.2087</td>\n",
       "      <td>28.39%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>11.54%</td>\n",
       "      <td>5.2407</td>\n",
       "      <td>27.81%</td>\n",
       "      <td>11.76%</td>\n",
       "      <td>5.2011</td>\n",
       "      <td>27.78%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>12.03%</td>\n",
       "      <td>5.1920</td>\n",
       "      <td>28.61%</td>\n",
       "      <td>12.37%</td>\n",
       "      <td>5.1767</td>\n",
       "      <td>28.87%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>12.20%</td>\n",
       "      <td>5.1411</td>\n",
       "      <td>29.37%</td>\n",
       "      <td>12.36%</td>\n",
       "      <td>5.1722</td>\n",
       "      <td>28.84%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>12.58%</td>\n",
       "      <td>5.0972</td>\n",
       "      <td>30.17%</td>\n",
       "      <td>12.48%</td>\n",
       "      <td>5.1835</td>\n",
       "      <td>28.84%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>12.72%</td>\n",
       "      <td>5.0822</td>\n",
       "      <td>30.43%</td>\n",
       "      <td>13.02%</td>\n",
       "      <td>5.1654</td>\n",
       "      <td>29.20%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>13.28%</td>\n",
       "      <td>5.0431</td>\n",
       "      <td>31.16%</td>\n",
       "      <td>12.65%</td>\n",
       "      <td>5.1900</td>\n",
       "      <td>29.57%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>13.33%</td>\n",
       "      <td>5.0129</td>\n",
       "      <td>31.72%</td>\n",
       "      <td>12.87%</td>\n",
       "      <td>5.1855</td>\n",
       "      <td>29.25%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>13.54%</td>\n",
       "      <td>4.9855</td>\n",
       "      <td>32.03%</td>\n",
       "      <td>13.11%</td>\n",
       "      <td>5.1475</td>\n",
       "      <td>29.75%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>13.82%</td>\n",
       "      <td>4.9508</td>\n",
       "      <td>33.02%</td>\n",
       "      <td>12.80%</td>\n",
       "      <td>5.1892</td>\n",
       "      <td>29.44%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>14.32%</td>\n",
       "      <td>4.9188</td>\n",
       "      <td>33.46%</td>\n",
       "      <td>12.76%</td>\n",
       "      <td>5.2624</td>\n",
       "      <td>29.14%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>14.27%</td>\n",
       "      <td>4.8946</td>\n",
       "      <td>33.70%</td>\n",
       "      <td>13.20%</td>\n",
       "      <td>5.2896</td>\n",
       "      <td>29.35%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>14.44%</td>\n",
       "      <td>4.8748</td>\n",
       "      <td>34.12%</td>\n",
       "      <td>12.57%</td>\n",
       "      <td>5.3345</td>\n",
       "      <td>28.54%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>14.95%</td>\n",
       "      <td>4.8332</td>\n",
       "      <td>34.85%</td>\n",
       "      <td>12.82%</td>\n",
       "      <td>5.2656</td>\n",
       "      <td>28.88%</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>16.49%</td>\n",
       "      <td>4.6124</td>\n",
       "      <td>38.07%</td>\n",
       "      <td>13.83%</td>\n",
       "      <td>5.1617</td>\n",
       "      <td>30.08%</td>\n",
       "      <td>0.00025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>17.48%</td>\n",
       "      <td>4.4545</td>\n",
       "      <td>39.94%</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>5.2062</td>\n",
       "      <td>30.02%</td>\n",
       "      <td>0.00025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>17.59%</td>\n",
       "      <td>4.4270</td>\n",
       "      <td>40.21%</td>\n",
       "      <td>13.90%</td>\n",
       "      <td>5.1390</td>\n",
       "      <td>30.10%</td>\n",
       "      <td>0.00025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch Accuracy (Train)  Loss (Train) Top-5 Accuracy (Train)  \\\n",
       "0       1            1.14%        7.0250                  4.61%   \n",
       "1       2            1.94%        6.5656                  7.16%   \n",
       "2       3            3.71%        6.3454                 10.48%   \n",
       "3       4            5.42%        6.1176                 13.90%   \n",
       "4       5            7.28%        5.8652                 18.11%   \n",
       "5       6            8.65%        5.6972                 21.04%   \n",
       "6       7            9.53%        5.5507                 23.32%   \n",
       "7       8           10.38%        5.4444                 24.88%   \n",
       "8       9           10.69%        5.3726                 25.85%   \n",
       "9      10           11.04%        5.3096                 26.82%   \n",
       "10     11           11.54%        5.2407                 27.81%   \n",
       "11     12           12.03%        5.1920                 28.61%   \n",
       "12     13           12.20%        5.1411                 29.37%   \n",
       "13     14           12.58%        5.0972                 30.17%   \n",
       "14     15           12.72%        5.0822                 30.43%   \n",
       "15     16           13.28%        5.0431                 31.16%   \n",
       "16     17           13.33%        5.0129                 31.72%   \n",
       "17     18           13.54%        4.9855                 32.03%   \n",
       "18     19           13.82%        4.9508                 33.02%   \n",
       "19     20           14.32%        4.9188                 33.46%   \n",
       "20     21           14.27%        4.8946                 33.70%   \n",
       "21     22           14.44%        4.8748                 34.12%   \n",
       "22     23           14.95%        4.8332                 34.85%   \n",
       "23     24           16.49%        4.6124                 38.07%   \n",
       "24     25           17.48%        4.4545                 39.94%   \n",
       "25     26           17.59%        4.4270                 40.21%   \n",
       "\n",
       "   Accuracy (Val)  Loss (Val) Top-5 Accuracy (Val)  Learning Rate  \n",
       "0           1.58%      6.8749                6.06%        0.00050  \n",
       "1           3.67%      6.3865               10.37%        0.00050  \n",
       "2           5.68%      6.0843               14.17%        0.00050  \n",
       "3           8.07%      5.8341               18.83%        0.00050  \n",
       "4           9.35%      5.6321               22.19%        0.00050  \n",
       "5          10.29%      5.5249               23.98%        0.00050  \n",
       "6          10.78%      5.3468               25.55%        0.00050  \n",
       "7          11.08%      5.2757               26.82%        0.00050  \n",
       "8          12.04%      5.2389               28.29%        0.00050  \n",
       "9          12.23%      5.2087               28.39%        0.00050  \n",
       "10         11.76%      5.2011               27.78%        0.00050  \n",
       "11         12.37%      5.1767               28.87%        0.00050  \n",
       "12         12.36%      5.1722               28.84%        0.00050  \n",
       "13         12.48%      5.1835               28.84%        0.00050  \n",
       "14         13.02%      5.1654               29.20%        0.00050  \n",
       "15         12.65%      5.1900               29.57%        0.00050  \n",
       "16         12.87%      5.1855               29.25%        0.00050  \n",
       "17         13.11%      5.1475               29.75%        0.00050  \n",
       "18         12.80%      5.1892               29.44%        0.00050  \n",
       "19         12.76%      5.2624               29.14%        0.00050  \n",
       "20         13.20%      5.2896               29.35%        0.00050  \n",
       "21         12.57%      5.3345               28.54%        0.00050  \n",
       "22         12.82%      5.2656               28.88%        0.00050  \n",
       "23         13.83%      5.1617               30.08%        0.00025  \n",
       "24         13.94%      5.2062               30.02%        0.00025  \n",
       "25         13.90%      5.1390               30.10%        0.00025  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m3_2= pd.read_csv(\"C:/Users/josej/Downloads/training_results_modelo3_2.csv\")\n",
    "df_m3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" inputs = tf.keras.Input(shape=(8, 8, 12))\n",
    "# Primera capa convolucional\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(inputs)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "# Bloque residual sencillo\n",
    "def res_block(x, filters, kernel_size=3):\n",
    "    shortcut = x\n",
    "    y = tf.keras.layers.Conv2D(filters, kernel_size, activation='relu', padding='same')(x)\n",
    "    y = tf.keras.layers.BatchNormalization()(y)\n",
    "    y = tf.keras.layers.Conv2D(filters, kernel_size, activation=None, padding='same')(y)\n",
    "    y = tf.keras.layers.BatchNormalization()(y)\n",
    "    y = tf.keras.layers.Add()([shortcut, y])\n",
    "    y = tf.keras.layers.Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "# Agregar un único bloque residual para no sobrecomplicar el modelo\n",
    "x = res_block(x, 128)\n",
    "\n",
    "\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "# Usar Flatten para preservar la información espacial\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# Definir focal loss para clasificación sparse\n",
    "\n",
    "def sparse_categorical_focal_loss(gamma=2.0, alpha=0.25, num_classes=None):\n",
    "    \"\"\n",
    "    Implementa focal loss para clasificación multiclase con etiquetas sparse.\n",
    "    Se convierte la etiqueta sparse a one-hot para el cálculo.\n",
    "    \"\"\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        # Convertir a one-hot\n",
    "        y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), depth=num_classes)\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        cross_entropy = -y_true_one_hot * tf.math.log(y_pred)\n",
    "        weight = alpha * tf.pow(1 - y_pred, gamma)\n",
    "        loss = weight * cross_entropy\n",
    "        return tf.reduce_mean(tf.reduce_sum(loss, axis=-1))\n",
    "    return focal_loss\n",
    "loss_fn = sparse_categorical_focal_loss(gamma=2.0, alpha=0.25, num_classes=num_classes)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name=\"top5_accuracy\")\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,  \n",
    "    batch_size=8,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001, verbose=1)\n",
    "    ]\n",
    ")\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Accuracy (Train)</th>\n",
       "      <th>Loss (Train)</th>\n",
       "      <th>Top-5 Accuracy (Train)</th>\n",
       "      <th>Accuracy (Val)</th>\n",
       "      <th>Loss (Val)</th>\n",
       "      <th>Top-5 Accuracy (Val)</th>\n",
       "      <th>Learning Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.31%</td>\n",
       "      <td>1.7689</td>\n",
       "      <td>5.10%</td>\n",
       "      <td>2.33%</td>\n",
       "      <td>1.6276</td>\n",
       "      <td>7.82%</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.54%</td>\n",
       "      <td>1.6272</td>\n",
       "      <td>8.06%</td>\n",
       "      <td>3.82%</td>\n",
       "      <td>1.5724</td>\n",
       "      <td>11.36%</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.16%</td>\n",
       "      <td>1.5777</td>\n",
       "      <td>11.36%</td>\n",
       "      <td>6.19%</td>\n",
       "      <td>1.5133</td>\n",
       "      <td>15.52%</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5.56%</td>\n",
       "      <td>1.5238</td>\n",
       "      <td>14.94%</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>1.4395</td>\n",
       "      <td>20.55%</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7.39%</td>\n",
       "      <td>1.4671</td>\n",
       "      <td>18.60%</td>\n",
       "      <td>9.72%</td>\n",
       "      <td>1.3943</td>\n",
       "      <td>23.31%</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>8.18%</td>\n",
       "      <td>1.4301</td>\n",
       "      <td>20.81%</td>\n",
       "      <td>10.48%</td>\n",
       "      <td>1.3606</td>\n",
       "      <td>24.07%</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>8.58%</td>\n",
       "      <td>1.4047</td>\n",
       "      <td>21.80%</td>\n",
       "      <td>10.41%</td>\n",
       "      <td>1.3589</td>\n",
       "      <td>25.13%</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>9.39%</td>\n",
       "      <td>1.3792</td>\n",
       "      <td>23.36%</td>\n",
       "      <td>10.66%</td>\n",
       "      <td>1.3279</td>\n",
       "      <td>25.38%</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>9.89%</td>\n",
       "      <td>1.3636</td>\n",
       "      <td>24.14%</td>\n",
       "      <td>11.14%</td>\n",
       "      <td>1.3186</td>\n",
       "      <td>26.42%</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>9.92%</td>\n",
       "      <td>1.3465</td>\n",
       "      <td>25.09%</td>\n",
       "      <td>11.22%</td>\n",
       "      <td>1.3298</td>\n",
       "      <td>25.87%</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>10.44%</td>\n",
       "      <td>1.3333</td>\n",
       "      <td>25.84%</td>\n",
       "      <td>11.41%</td>\n",
       "      <td>1.3140</td>\n",
       "      <td>26.72%</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>10.84%</td>\n",
       "      <td>1.3195</td>\n",
       "      <td>26.70%</td>\n",
       "      <td>11.69%</td>\n",
       "      <td>1.3245</td>\n",
       "      <td>27.15%</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>11.08%</td>\n",
       "      <td>1.3091</td>\n",
       "      <td>27.39%</td>\n",
       "      <td>11.64%</td>\n",
       "      <td>1.3190</td>\n",
       "      <td>26.71%</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>11.50%</td>\n",
       "      <td>1.2995</td>\n",
       "      <td>27.86%</td>\n",
       "      <td>11.52%</td>\n",
       "      <td>1.3315</td>\n",
       "      <td>26.77%</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>11.66%</td>\n",
       "      <td>1.2902</td>\n",
       "      <td>28.48%</td>\n",
       "      <td>11.78%</td>\n",
       "      <td>1.3388</td>\n",
       "      <td>26.31%</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>11.73%</td>\n",
       "      <td>1.2791</td>\n",
       "      <td>29.10%</td>\n",
       "      <td>11.99%</td>\n",
       "      <td>1.3332</td>\n",
       "      <td>27.05%</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>13.33%</td>\n",
       "      <td>1.2193</td>\n",
       "      <td>31.74%</td>\n",
       "      <td>12.57%</td>\n",
       "      <td>1.3116</td>\n",
       "      <td>27.58%</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>14.05%</td>\n",
       "      <td>1.1866</td>\n",
       "      <td>33.18%</td>\n",
       "      <td>12.44%</td>\n",
       "      <td>1.3172</td>\n",
       "      <td>27.43%</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>14.16%</td>\n",
       "      <td>1.1769</td>\n",
       "      <td>33.77%</td>\n",
       "      <td>12.61%</td>\n",
       "      <td>1.3195</td>\n",
       "      <td>27.52%</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>14.52%</td>\n",
       "      <td>1.1688</td>\n",
       "      <td>34.42%</td>\n",
       "      <td>12.36%</td>\n",
       "      <td>1.3360</td>\n",
       "      <td>26.94%</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>14.65%</td>\n",
       "      <td>1.1626</td>\n",
       "      <td>34.82%</td>\n",
       "      <td>12.49%</td>\n",
       "      <td>1.3430</td>\n",
       "      <td>27.13%</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>14.73%</td>\n",
       "      <td>1.1577</td>\n",
       "      <td>35.18%</td>\n",
       "      <td>12.49%</td>\n",
       "      <td>1.3667</td>\n",
       "      <td>27.09%</td>\n",
       "      <td>0.000250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>15.84%</td>\n",
       "      <td>1.1196</td>\n",
       "      <td>37.17%</td>\n",
       "      <td>12.89%</td>\n",
       "      <td>1.3410</td>\n",
       "      <td>27.26%</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch Accuracy (Train)  Loss (Train) Top-5 Accuracy (Train)  \\\n",
       "0       1            1.31%        1.7689                  5.10%   \n",
       "1       2            2.54%        1.6272                  8.06%   \n",
       "2       3            4.16%        1.5777                 11.36%   \n",
       "3       4            5.56%        1.5238                 14.94%   \n",
       "4       5            7.39%        1.4671                 18.60%   \n",
       "5       6            8.18%        1.4301                 20.81%   \n",
       "6       7            8.58%        1.4047                 21.80%   \n",
       "7       8            9.39%        1.3792                 23.36%   \n",
       "8       9            9.89%        1.3636                 24.14%   \n",
       "9      10            9.92%        1.3465                 25.09%   \n",
       "10     11           10.44%        1.3333                 25.84%   \n",
       "11     12           10.84%        1.3195                 26.70%   \n",
       "12     13           11.08%        1.3091                 27.39%   \n",
       "13     14           11.50%        1.2995                 27.86%   \n",
       "14     15           11.66%        1.2902                 28.48%   \n",
       "15     16           11.73%        1.2791                 29.10%   \n",
       "16     17           13.33%        1.2193                 31.74%   \n",
       "17     18           14.05%        1.1866                 33.18%   \n",
       "18     19           14.16%        1.1769                 33.77%   \n",
       "19     20           14.52%        1.1688                 34.42%   \n",
       "20     21           14.65%        1.1626                 34.82%   \n",
       "21     22           14.73%        1.1577                 35.18%   \n",
       "22     23           15.84%        1.1196                 37.17%   \n",
       "\n",
       "   Accuracy (Val)  Loss (Val) Top-5 Accuracy (Val)  Learning Rate  \n",
       "0           2.33%      1.6276                7.82%       0.000500  \n",
       "1           3.82%      1.5724               11.36%       0.000500  \n",
       "2           6.19%      1.5133               15.52%       0.000500  \n",
       "3           8.39%      1.4395               20.55%       0.000500  \n",
       "4           9.72%      1.3943               23.31%       0.000500  \n",
       "5          10.48%      1.3606               24.07%       0.000500  \n",
       "6          10.41%      1.3589               25.13%       0.000500  \n",
       "7          10.66%      1.3279               25.38%       0.000500  \n",
       "8          11.14%      1.3186               26.42%       0.000500  \n",
       "9          11.22%      1.3298               25.87%       0.000500  \n",
       "10         11.41%      1.3140               26.72%       0.000500  \n",
       "11         11.69%      1.3245               27.15%       0.000500  \n",
       "12         11.64%      1.3190               26.71%       0.000500  \n",
       "13         11.52%      1.3315               26.77%       0.000500  \n",
       "14         11.78%      1.3388               26.31%       0.000500  \n",
       "15         11.99%      1.3332               27.05%       0.000500  \n",
       "16         12.57%      1.3116               27.58%       0.000250  \n",
       "17         12.44%      1.3172               27.43%       0.000250  \n",
       "18         12.61%      1.3195               27.52%       0.000250  \n",
       "19         12.36%      1.3360               26.94%       0.000250  \n",
       "20         12.49%      1.3430               27.13%       0.000250  \n",
       "21         12.49%      1.3667               27.09%       0.000250  \n",
       "22         12.89%      1.3410               27.26%       0.000125  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m3_3= pd.read_csv(\"C:/Users/josej/Downloads/training_results_modelo3_3.csv\")\n",
    "df_m3_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuarta arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" inputs = tf.keras.Input(shape=(8, 8, 17))\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(inputs)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "def res_block(x, filters):\n",
    "    shortcut = x\n",
    "    y = tf.keras.layers.Conv2D(filters, (3, 3), activation='relu', padding='same')(x)\n",
    "    y = tf.keras.layers.BatchNormalization()(y)\n",
    "    y = tf.keras.layers.Conv2D(filters, (3, 3), activation=None, padding='same')(y)\n",
    "    y = tf.keras.layers.BatchNormalization()(y)\n",
    "    y = tf.keras.layers.Add()([shortcut, y])\n",
    "    y = tf.keras.layers.Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "x = res_block(x, 128)\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "output_from = tf.keras.layers.Dense(64, activation='softmax', name=\"from_square\")(x)\n",
    "output_to = tf.keras.layers.Dense(64, activation='softmax', name=\"to_square\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=[output_from, output_to])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "    loss={\"from_square\": \"sparse_categorical_crossentropy\", \"to_square\": \"sparse_categorical_crossentropy\"},\n",
    "    metrics={\"from_square\": \"accuracy\", \"to_square\": \"accuracy\"}\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, {\"from_square\": y_train_from, \"to_square\": y_train_to},\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_test, {\"from_square\": y_test_from, \"to_square\": y_test_to}),\n",
    "    verbose=2,  # Reduce la cantidad de información impresa\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001, verbose=1)\n",
    "    ]\n",
    ") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>From Square Accuracy (Train)</th>\n",
       "      <th>From Square Loss (Train)</th>\n",
       "      <th>To Square Accuracy (Train)</th>\n",
       "      <th>To Square Loss (Train)</th>\n",
       "      <th>Loss (Train)</th>\n",
       "      <th>From Square Accuracy (Val)</th>\n",
       "      <th>From Square Loss (Val)</th>\n",
       "      <th>To Square Accuracy (Val)</th>\n",
       "      <th>To Square Loss (Val)</th>\n",
       "      <th>Loss (Val)</th>\n",
       "      <th>Learning Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>19.87%</td>\n",
       "      <td>2.9614</td>\n",
       "      <td>8.75%</td>\n",
       "      <td>3.6554</td>\n",
       "      <td>6.7163</td>\n",
       "      <td>37.09%</td>\n",
       "      <td>2.2358</td>\n",
       "      <td>13.90%</td>\n",
       "      <td>3.4131</td>\n",
       "      <td>5.7612</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>36.63%</td>\n",
       "      <td>2.2198</td>\n",
       "      <td>14.17%</td>\n",
       "      <td>3.4169</td>\n",
       "      <td>5.7554</td>\n",
       "      <td>41.50%</td>\n",
       "      <td>1.9332</td>\n",
       "      <td>17.42%</td>\n",
       "      <td>3.2614</td>\n",
       "      <td>5.3170</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>40.58%</td>\n",
       "      <td>2.0016</td>\n",
       "      <td>16.82%</td>\n",
       "      <td>3.2964</td>\n",
       "      <td>5.4220</td>\n",
       "      <td>43.61%</td>\n",
       "      <td>1.7969</td>\n",
       "      <td>19.37%</td>\n",
       "      <td>3.1498</td>\n",
       "      <td>5.0712</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>42.47%</td>\n",
       "      <td>1.8932</td>\n",
       "      <td>18.61%</td>\n",
       "      <td>3.2046</td>\n",
       "      <td>5.2235</td>\n",
       "      <td>44.86%</td>\n",
       "      <td>1.7576</td>\n",
       "      <td>20.81%</td>\n",
       "      <td>3.0796</td>\n",
       "      <td>4.9628</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>43.96%</td>\n",
       "      <td>1.8209</td>\n",
       "      <td>20.06%</td>\n",
       "      <td>3.1310</td>\n",
       "      <td>5.0791</td>\n",
       "      <td>45.54%</td>\n",
       "      <td>1.6829</td>\n",
       "      <td>22.16%</td>\n",
       "      <td>3.0100</td>\n",
       "      <td>4.8199</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>44.75%</td>\n",
       "      <td>1.7728</td>\n",
       "      <td>21.40%</td>\n",
       "      <td>3.0698</td>\n",
       "      <td>4.9712</td>\n",
       "      <td>45.45%</td>\n",
       "      <td>1.6698</td>\n",
       "      <td>23.16%</td>\n",
       "      <td>2.9616</td>\n",
       "      <td>4.7594</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>45.51%</td>\n",
       "      <td>1.7335</td>\n",
       "      <td>22.53%</td>\n",
       "      <td>3.0139</td>\n",
       "      <td>4.8779</td>\n",
       "      <td>45.68%</td>\n",
       "      <td>1.6493</td>\n",
       "      <td>24.13%</td>\n",
       "      <td>2.9145</td>\n",
       "      <td>4.6931</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>46.45%</td>\n",
       "      <td>1.7003</td>\n",
       "      <td>23.58%</td>\n",
       "      <td>2.9673</td>\n",
       "      <td>4.7987</td>\n",
       "      <td>46.22%</td>\n",
       "      <td>1.6395</td>\n",
       "      <td>25.37%</td>\n",
       "      <td>2.8699</td>\n",
       "      <td>4.6405</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>47.11%</td>\n",
       "      <td>1.6695</td>\n",
       "      <td>24.51%</td>\n",
       "      <td>2.9211</td>\n",
       "      <td>4.7239</td>\n",
       "      <td>46.37%</td>\n",
       "      <td>1.6545</td>\n",
       "      <td>25.72%</td>\n",
       "      <td>2.8513</td>\n",
       "      <td>4.6389</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>47.79%</td>\n",
       "      <td>1.6476</td>\n",
       "      <td>25.27%</td>\n",
       "      <td>2.8862</td>\n",
       "      <td>4.6679</td>\n",
       "      <td>46.34%</td>\n",
       "      <td>1.6360</td>\n",
       "      <td>25.98%</td>\n",
       "      <td>2.8327</td>\n",
       "      <td>4.6020</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>48.31%</td>\n",
       "      <td>1.6256</td>\n",
       "      <td>26.06%</td>\n",
       "      <td>2.8520</td>\n",
       "      <td>4.6122</td>\n",
       "      <td>46.92%</td>\n",
       "      <td>1.6169</td>\n",
       "      <td>26.73%</td>\n",
       "      <td>2.8094</td>\n",
       "      <td>4.5595</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>48.74%</td>\n",
       "      <td>1.6061</td>\n",
       "      <td>26.61%</td>\n",
       "      <td>2.8193</td>\n",
       "      <td>4.5609</td>\n",
       "      <td>46.89%</td>\n",
       "      <td>1.6137</td>\n",
       "      <td>26.62%</td>\n",
       "      <td>2.8011</td>\n",
       "      <td>4.5490</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>49.24%</td>\n",
       "      <td>1.5869</td>\n",
       "      <td>27.29%</td>\n",
       "      <td>2.7933</td>\n",
       "      <td>4.5169</td>\n",
       "      <td>46.99%</td>\n",
       "      <td>1.6275</td>\n",
       "      <td>27.06%</td>\n",
       "      <td>2.8080</td>\n",
       "      <td>4.5711</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>49.67%</td>\n",
       "      <td>1.5716</td>\n",
       "      <td>27.76%</td>\n",
       "      <td>2.7678</td>\n",
       "      <td>4.4767</td>\n",
       "      <td>47.42%</td>\n",
       "      <td>1.6137</td>\n",
       "      <td>27.59%</td>\n",
       "      <td>2.7705</td>\n",
       "      <td>4.5205</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>50.16%</td>\n",
       "      <td>1.5553</td>\n",
       "      <td>28.18%</td>\n",
       "      <td>2.7442</td>\n",
       "      <td>4.4375</td>\n",
       "      <td>47.37%</td>\n",
       "      <td>1.6096</td>\n",
       "      <td>27.74%</td>\n",
       "      <td>2.7656</td>\n",
       "      <td>4.5129</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch From Square Accuracy (Train)  From Square Loss (Train)  \\\n",
       "0       1                       19.87%                    2.9614   \n",
       "1       2                       36.63%                    2.2198   \n",
       "2       3                       40.58%                    2.0016   \n",
       "3       4                       42.47%                    1.8932   \n",
       "4       5                       43.96%                    1.8209   \n",
       "5       6                       44.75%                    1.7728   \n",
       "6       7                       45.51%                    1.7335   \n",
       "7       8                       46.45%                    1.7003   \n",
       "8       9                       47.11%                    1.6695   \n",
       "9      10                       47.79%                    1.6476   \n",
       "10     11                       48.31%                    1.6256   \n",
       "11     12                       48.74%                    1.6061   \n",
       "12     13                       49.24%                    1.5869   \n",
       "13     14                       49.67%                    1.5716   \n",
       "14     15                       50.16%                    1.5553   \n",
       "\n",
       "   To Square Accuracy (Train)  To Square Loss (Train)  Loss (Train)  \\\n",
       "0                       8.75%                  3.6554        6.7163   \n",
       "1                      14.17%                  3.4169        5.7554   \n",
       "2                      16.82%                  3.2964        5.4220   \n",
       "3                      18.61%                  3.2046        5.2235   \n",
       "4                      20.06%                  3.1310        5.0791   \n",
       "5                      21.40%                  3.0698        4.9712   \n",
       "6                      22.53%                  3.0139        4.8779   \n",
       "7                      23.58%                  2.9673        4.7987   \n",
       "8                      24.51%                  2.9211        4.7239   \n",
       "9                      25.27%                  2.8862        4.6679   \n",
       "10                     26.06%                  2.8520        4.6122   \n",
       "11                     26.61%                  2.8193        4.5609   \n",
       "12                     27.29%                  2.7933        4.5169   \n",
       "13                     27.76%                  2.7678        4.4767   \n",
       "14                     28.18%                  2.7442        4.4375   \n",
       "\n",
       "   From Square Accuracy (Val)  From Square Loss (Val)  \\\n",
       "0                      37.09%                  2.2358   \n",
       "1                      41.50%                  1.9332   \n",
       "2                      43.61%                  1.7969   \n",
       "3                      44.86%                  1.7576   \n",
       "4                      45.54%                  1.6829   \n",
       "5                      45.45%                  1.6698   \n",
       "6                      45.68%                  1.6493   \n",
       "7                      46.22%                  1.6395   \n",
       "8                      46.37%                  1.6545   \n",
       "9                      46.34%                  1.6360   \n",
       "10                     46.92%                  1.6169   \n",
       "11                     46.89%                  1.6137   \n",
       "12                     46.99%                  1.6275   \n",
       "13                     47.42%                  1.6137   \n",
       "14                     47.37%                  1.6096   \n",
       "\n",
       "   To Square Accuracy (Val)  To Square Loss (Val)  Loss (Val)  Learning Rate  \n",
       "0                    13.90%                3.4131      5.7612         0.0005  \n",
       "1                    17.42%                3.2614      5.3170         0.0005  \n",
       "2                    19.37%                3.1498      5.0712         0.0005  \n",
       "3                    20.81%                3.0796      4.9628         0.0005  \n",
       "4                    22.16%                3.0100      4.8199         0.0005  \n",
       "5                    23.16%                2.9616      4.7594         0.0005  \n",
       "6                    24.13%                2.9145      4.6931         0.0005  \n",
       "7                    25.37%                2.8699      4.6405         0.0005  \n",
       "8                    25.72%                2.8513      4.6389         0.0005  \n",
       "9                    25.98%                2.8327      4.6020         0.0005  \n",
       "10                   26.73%                2.8094      4.5595         0.0005  \n",
       "11                   26.62%                2.8011      4.5490         0.0005  \n",
       "12                   27.06%                2.8080      4.5711         0.0005  \n",
       "13                   27.59%                2.7705      4.5205         0.0005  \n",
       "14                   27.74%                2.7656      4.5129         0.0005  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m4_1 = pd.read_csv(\"C:/Users/josej/Downloads/training_results_modelo4_1.csv\")\n",
    "df_m4_1.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>From Square Accuracy (Train)</th>\n",
       "      <th>From Square Loss (Train)</th>\n",
       "      <th>To Square Accuracy (Train)</th>\n",
       "      <th>To Square Loss (Train)</th>\n",
       "      <th>Loss (Train)</th>\n",
       "      <th>From Square Accuracy (Val)</th>\n",
       "      <th>From Square Loss (Val)</th>\n",
       "      <th>To Square Accuracy (Val)</th>\n",
       "      <th>To Square Loss (Val)</th>\n",
       "      <th>Loss (Val)</th>\n",
       "      <th>Learning Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>31.01%</td>\n",
       "      <td>2.4478</td>\n",
       "      <td>13.19%</td>\n",
       "      <td>3.4669</td>\n",
       "      <td>6.0272</td>\n",
       "      <td>42.42%</td>\n",
       "      <td>1.9104</td>\n",
       "      <td>19.44%</td>\n",
       "      <td>3.1857</td>\n",
       "      <td>5.2203</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>41.63%</td>\n",
       "      <td>1.8980</td>\n",
       "      <td>19.49%</td>\n",
       "      <td>3.1739</td>\n",
       "      <td>5.1983</td>\n",
       "      <td>45.28%</td>\n",
       "      <td>1.8712</td>\n",
       "      <td>23.69%</td>\n",
       "      <td>3.0490</td>\n",
       "      <td>5.0484</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>43.44%</td>\n",
       "      <td>1.7875</td>\n",
       "      <td>22.55%</td>\n",
       "      <td>3.0274</td>\n",
       "      <td>4.9455</td>\n",
       "      <td>45.99%</td>\n",
       "      <td>2.0149</td>\n",
       "      <td>26.30%</td>\n",
       "      <td>3.0288</td>\n",
       "      <td>5.1750</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>44.52%</td>\n",
       "      <td>1.7315</td>\n",
       "      <td>24.56%</td>\n",
       "      <td>2.9321</td>\n",
       "      <td>4.7970</td>\n",
       "      <td>46.85%</td>\n",
       "      <td>2.3986</td>\n",
       "      <td>27.67%</td>\n",
       "      <td>3.3412</td>\n",
       "      <td>5.8737</td>\n",
       "      <td>0.00050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>45.28%</td>\n",
       "      <td>1.6929</td>\n",
       "      <td>25.81%</td>\n",
       "      <td>2.8695</td>\n",
       "      <td>4.6973</td>\n",
       "      <td>46.81%</td>\n",
       "      <td>2.5007</td>\n",
       "      <td>28.70%</td>\n",
       "      <td>3.2846</td>\n",
       "      <td>5.9198</td>\n",
       "      <td>0.00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>47.62%</td>\n",
       "      <td>1.5795</td>\n",
       "      <td>28.31%</td>\n",
       "      <td>2.7356</td>\n",
       "      <td>4.4185</td>\n",
       "      <td>48.97%</td>\n",
       "      <td>2.4426</td>\n",
       "      <td>30.58%</td>\n",
       "      <td>2.9464</td>\n",
       "      <td>5.4772</td>\n",
       "      <td>0.00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>48.16%</td>\n",
       "      <td>1.5592</td>\n",
       "      <td>28.89%</td>\n",
       "      <td>2.7006</td>\n",
       "      <td>4.3438</td>\n",
       "      <td>48.93%</td>\n",
       "      <td>2.0077</td>\n",
       "      <td>30.84%</td>\n",
       "      <td>2.9104</td>\n",
       "      <td>4.9987</td>\n",
       "      <td>0.00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>48.59%</td>\n",
       "      <td>1.5465</td>\n",
       "      <td>29.44%</td>\n",
       "      <td>2.6788</td>\n",
       "      <td>4.3044</td>\n",
       "      <td>48.88%</td>\n",
       "      <td>2.6333</td>\n",
       "      <td>31.07%</td>\n",
       "      <td>3.1684</td>\n",
       "      <td>5.8791</td>\n",
       "      <td>0.00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>48.81%</td>\n",
       "      <td>1.5375</td>\n",
       "      <td>29.65%</td>\n",
       "      <td>2.6638</td>\n",
       "      <td>4.2779</td>\n",
       "      <td>49.18%</td>\n",
       "      <td>2.1795</td>\n",
       "      <td>31.08%</td>\n",
       "      <td>2.9438</td>\n",
       "      <td>5.1986</td>\n",
       "      <td>0.00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>49.04%</td>\n",
       "      <td>1.5299</td>\n",
       "      <td>30.03%</td>\n",
       "      <td>2.6483</td>\n",
       "      <td>4.2531</td>\n",
       "      <td>49.17%</td>\n",
       "      <td>1.6833</td>\n",
       "      <td>31.35%</td>\n",
       "      <td>2.7134</td>\n",
       "      <td>4.4710</td>\n",
       "      <td>0.00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>49.28%</td>\n",
       "      <td>1.5212</td>\n",
       "      <td>30.33%</td>\n",
       "      <td>2.6329</td>\n",
       "      <td>4.2281</td>\n",
       "      <td>49.29%</td>\n",
       "      <td>1.5210</td>\n",
       "      <td>31.55%</td>\n",
       "      <td>2.5763</td>\n",
       "      <td>4.1704</td>\n",
       "      <td>0.00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>49.55%</td>\n",
       "      <td>1.5123</td>\n",
       "      <td>30.56%</td>\n",
       "      <td>2.6207</td>\n",
       "      <td>4.2063</td>\n",
       "      <td>49.33%</td>\n",
       "      <td>2.5929</td>\n",
       "      <td>31.72%</td>\n",
       "      <td>3.1982</td>\n",
       "      <td>5.8635</td>\n",
       "      <td>0.00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>49.75%</td>\n",
       "      <td>1.5057</td>\n",
       "      <td>30.76%</td>\n",
       "      <td>2.6098</td>\n",
       "      <td>4.1880</td>\n",
       "      <td>49.34%</td>\n",
       "      <td>1.9808</td>\n",
       "      <td>31.73%</td>\n",
       "      <td>2.8708</td>\n",
       "      <td>4.9235</td>\n",
       "      <td>0.00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>49.89%</td>\n",
       "      <td>1.5012</td>\n",
       "      <td>30.97%</td>\n",
       "      <td>2.5983</td>\n",
       "      <td>4.1716</td>\n",
       "      <td>49.37%</td>\n",
       "      <td>1.6615</td>\n",
       "      <td>31.84%</td>\n",
       "      <td>2.6597</td>\n",
       "      <td>4.3928</td>\n",
       "      <td>0.00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>50.49%</td>\n",
       "      <td>1.4756</td>\n",
       "      <td>31.52%</td>\n",
       "      <td>2.5690</td>\n",
       "      <td>4.1124</td>\n",
       "      <td>49.64%</td>\n",
       "      <td>1.9827</td>\n",
       "      <td>31.95%</td>\n",
       "      <td>2.8733</td>\n",
       "      <td>4.9210</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>50.69%</td>\n",
       "      <td>1.4680</td>\n",
       "      <td>31.76%</td>\n",
       "      <td>2.5573</td>\n",
       "      <td>4.0892</td>\n",
       "      <td>49.68%</td>\n",
       "      <td>1.6926</td>\n",
       "      <td>32.18%</td>\n",
       "      <td>2.6262</td>\n",
       "      <td>4.3813</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>50.81%</td>\n",
       "      <td>1.4657</td>\n",
       "      <td>31.92%</td>\n",
       "      <td>2.5514</td>\n",
       "      <td>4.0791</td>\n",
       "      <td>49.58%</td>\n",
       "      <td>1.6328</td>\n",
       "      <td>32.17%</td>\n",
       "      <td>2.6704</td>\n",
       "      <td>4.3644</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>50.88%</td>\n",
       "      <td>1.4610</td>\n",
       "      <td>32.12%</td>\n",
       "      <td>2.5445</td>\n",
       "      <td>4.0665</td>\n",
       "      <td>49.93%</td>\n",
       "      <td>2.5972</td>\n",
       "      <td>32.37%</td>\n",
       "      <td>3.2165</td>\n",
       "      <td>5.8741</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>50.99%</td>\n",
       "      <td>1.4576</td>\n",
       "      <td>32.17%</td>\n",
       "      <td>2.5383</td>\n",
       "      <td>4.0564</td>\n",
       "      <td>49.81%</td>\n",
       "      <td>1.6657</td>\n",
       "      <td>32.39%</td>\n",
       "      <td>2.6802</td>\n",
       "      <td>4.4059</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>51.11%</td>\n",
       "      <td>1.4531</td>\n",
       "      <td>32.35%</td>\n",
       "      <td>2.5312</td>\n",
       "      <td>4.0441</td>\n",
       "      <td>49.68%</td>\n",
       "      <td>2.7125</td>\n",
       "      <td>32.37%</td>\n",
       "      <td>3.3749</td>\n",
       "      <td>6.1469</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>51.23%</td>\n",
       "      <td>1.4518</td>\n",
       "      <td>32.43%</td>\n",
       "      <td>2.5277</td>\n",
       "      <td>4.0388</td>\n",
       "      <td>49.71%</td>\n",
       "      <td>2.0801</td>\n",
       "      <td>32.31%</td>\n",
       "      <td>2.9328</td>\n",
       "      <td>5.0719</td>\n",
       "      <td>0.00010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Epoch From Square Accuracy (Train)  From Square Loss (Train)  \\\n",
       "0       1                       31.01%                    2.4478   \n",
       "1       2                       41.63%                    1.8980   \n",
       "2       3                       43.44%                    1.7875   \n",
       "3       4                       44.52%                    1.7315   \n",
       "4       5                       45.28%                    1.6929   \n",
       "5       6                       47.62%                    1.5795   \n",
       "6       7                       48.16%                    1.5592   \n",
       "7       8                       48.59%                    1.5465   \n",
       "8       9                       48.81%                    1.5375   \n",
       "9      10                       49.04%                    1.5299   \n",
       "10     11                       49.28%                    1.5212   \n",
       "11     12                       49.55%                    1.5123   \n",
       "12     13                       49.75%                    1.5057   \n",
       "13     14                       49.89%                    1.5012   \n",
       "14     15                       50.49%                    1.4756   \n",
       "15     16                       50.69%                    1.4680   \n",
       "16     17                       50.81%                    1.4657   \n",
       "17     18                       50.88%                    1.4610   \n",
       "18     19                       50.99%                    1.4576   \n",
       "19     20                       51.11%                    1.4531   \n",
       "20     21                       51.23%                    1.4518   \n",
       "\n",
       "   To Square Accuracy (Train)  To Square Loss (Train)  Loss (Train)  \\\n",
       "0                      13.19%                  3.4669        6.0272   \n",
       "1                      19.49%                  3.1739        5.1983   \n",
       "2                      22.55%                  3.0274        4.9455   \n",
       "3                      24.56%                  2.9321        4.7970   \n",
       "4                      25.81%                  2.8695        4.6973   \n",
       "5                      28.31%                  2.7356        4.4185   \n",
       "6                      28.89%                  2.7006        4.3438   \n",
       "7                      29.44%                  2.6788        4.3044   \n",
       "8                      29.65%                  2.6638        4.2779   \n",
       "9                      30.03%                  2.6483        4.2531   \n",
       "10                     30.33%                  2.6329        4.2281   \n",
       "11                     30.56%                  2.6207        4.2063   \n",
       "12                     30.76%                  2.6098        4.1880   \n",
       "13                     30.97%                  2.5983        4.1716   \n",
       "14                     31.52%                  2.5690        4.1124   \n",
       "15                     31.76%                  2.5573        4.0892   \n",
       "16                     31.92%                  2.5514        4.0791   \n",
       "17                     32.12%                  2.5445        4.0665   \n",
       "18                     32.17%                  2.5383        4.0564   \n",
       "19                     32.35%                  2.5312        4.0441   \n",
       "20                     32.43%                  2.5277        4.0388   \n",
       "\n",
       "   From Square Accuracy (Val)  From Square Loss (Val)  \\\n",
       "0                      42.42%                  1.9104   \n",
       "1                      45.28%                  1.8712   \n",
       "2                      45.99%                  2.0149   \n",
       "3                      46.85%                  2.3986   \n",
       "4                      46.81%                  2.5007   \n",
       "5                      48.97%                  2.4426   \n",
       "6                      48.93%                  2.0077   \n",
       "7                      48.88%                  2.6333   \n",
       "8                      49.18%                  2.1795   \n",
       "9                      49.17%                  1.6833   \n",
       "10                     49.29%                  1.5210   \n",
       "11                     49.33%                  2.5929   \n",
       "12                     49.34%                  1.9808   \n",
       "13                     49.37%                  1.6615   \n",
       "14                     49.64%                  1.9827   \n",
       "15                     49.68%                  1.6926   \n",
       "16                     49.58%                  1.6328   \n",
       "17                     49.93%                  2.5972   \n",
       "18                     49.81%                  1.6657   \n",
       "19                     49.68%                  2.7125   \n",
       "20                     49.71%                  2.0801   \n",
       "\n",
       "   To Square Accuracy (Val)  To Square Loss (Val)  Loss (Val)  Learning Rate  \n",
       "0                    19.44%                3.1857      5.2203        0.00050  \n",
       "1                    23.69%                3.0490      5.0484        0.00050  \n",
       "2                    26.30%                3.0288      5.1750        0.00050  \n",
       "3                    27.67%                3.3412      5.8737        0.00050  \n",
       "4                    28.70%                3.2846      5.9198        0.00015  \n",
       "5                    30.58%                2.9464      5.4772        0.00015  \n",
       "6                    30.84%                2.9104      4.9987        0.00015  \n",
       "7                    31.07%                3.1684      5.8791        0.00015  \n",
       "8                    31.08%                2.9438      5.1986        0.00015  \n",
       "9                    31.35%                2.7134      4.4710        0.00015  \n",
       "10                   31.55%                2.5763      4.1704        0.00015  \n",
       "11                   31.72%                3.1982      5.8635        0.00015  \n",
       "12                   31.73%                2.8708      4.9235        0.00015  \n",
       "13                   31.84%                2.6597      4.3928        0.00015  \n",
       "14                   31.95%                2.8733      4.9210        0.00010  \n",
       "15                   32.18%                2.6262      4.3813        0.00010  \n",
       "16                   32.17%                2.6704      4.3644        0.00010  \n",
       "17                   32.37%                3.2165      5.8741        0.00010  \n",
       "18                   32.39%                2.6802      4.4059        0.00010  \n",
       "19                   32.37%                3.3749      6.1469        0.00010  \n",
       "20                   32.31%                2.9328      5.0719        0.00010  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Paciencia aumentada a 10 ###\n",
    "import pandas as pd\n",
    "df4_2 = pd.read_csv('Resultados/training_results_modelo4_2.csv')\n",
    "df4_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quinta arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"inputs = tf.keras.Input(shape=(8, 8, 17))\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(inputs)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "def res_block(x, filters):\n",
    "    shortcut = x\n",
    "    y = tf.keras.layers.Conv2D(filters, (3, 3), activation='relu', padding='same')(x)\n",
    "    y = tf.keras.layers.BatchNormalization()(y)\n",
    "    y = tf.keras.layers.Conv2D(filters, (3, 3), activation=None, padding='same')(y)\n",
    "    y = tf.keras.layers.BatchNormalization()(y)\n",
    "    y = tf.keras.layers.Add()([shortcut, y])\n",
    "    y = tf.keras.layers.Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "x = res_block(x, 128)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "# Cabeza 1: from_square\n",
    "output_from = tf.keras.layers.Dense(64, activation='softmax', name=\"from_square\")(x)\n",
    "# Cabeza 2: to_square\n",
    "output_to = tf.keras.layers.Dense(64, activation='softmax', name=\"to_square\")(x)\n",
    "# Cabeza 3: position_eval\n",
    "output_eval = tf.keras.layers.Dense(1, activation='tanh', name=\"position_eval\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=[output_from, output_to, output_eval])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={\n",
    "        \"from_square\": \"sparse_categorical_crossentropy\",\n",
    "        \"to_square\": \"sparse_categorical_crossentropy\",\n",
    "        \"position_eval\": \"mse\"\n",
    "    },\n",
    "    metrics={\n",
    "        \"from_square\": \"accuracy\",\n",
    "        \"to_square\": \"accuracy\",\n",
    "        \"position_eval\": \"mae\"\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    {\n",
    "        \"from_square\": y_train_from,\n",
    "        \"to_square\": y_train_to,\n",
    "        \"position_eval\": y_train_eval\n",
    "    },\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(\n",
    "        X_test,\n",
    "        {\n",
    "            \"from_square\": y_test_from,\n",
    "            \"to_square\": y_test_to,\n",
    "            \"position_eval\": y_test_eval\n",
    "        }\n",
    "    ),\n",
    "    verbose=2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=10, restore_best_weights=True, verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001, verbose=1\n",
    "        )\n",
    "    ]\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>From Square Accuracy (Train)</th>\n",
       "      <th>From Square Loss (Train)</th>\n",
       "      <th>To Square Accuracy (Train)</th>\n",
       "      <th>To Square Loss (Train)</th>\n",
       "      <th>Loss (Train)</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>From Square Accuracy (Val)</th>\n",
       "      <th>To Square Accuracy (Val)</th>\n",
       "      <th>Position Eval MAE (Train)</th>\n",
       "      <th>Position Eval MAE (Val)</th>\n",
       "      <th>Loss (Val)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9.97%</td>\n",
       "      <td>3.4190</td>\n",
       "      <td>6.06%</td>\n",
       "      <td>3.7722</td>\n",
       "      <td>24.8948</td>\n",
       "      <td>0.001</td>\n",
       "      <td>19.21%</td>\n",
       "      <td>8.53%</td>\n",
       "      <td>2.7405</td>\n",
       "      <td>2.6667</td>\n",
       "      <td>24.1215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>21.25%</td>\n",
       "      <td>2.8075</td>\n",
       "      <td>9.42%</td>\n",
       "      <td>3.5953</td>\n",
       "      <td>23.4791</td>\n",
       "      <td>0.001</td>\n",
       "      <td>28.09%</td>\n",
       "      <td>11.87%</td>\n",
       "      <td>2.6356</td>\n",
       "      <td>2.7472</td>\n",
       "      <td>24.1544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>28.48%</td>\n",
       "      <td>2.4963</td>\n",
       "      <td>12.12%</td>\n",
       "      <td>3.4862</td>\n",
       "      <td>22.8679</td>\n",
       "      <td>0.001</td>\n",
       "      <td>34.21%</td>\n",
       "      <td>14.14%</td>\n",
       "      <td>2.5981</td>\n",
       "      <td>2.6242</td>\n",
       "      <td>22.9873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>32.86%</td>\n",
       "      <td>2.3187</td>\n",
       "      <td>13.74%</td>\n",
       "      <td>3.4173</td>\n",
       "      <td>22.4888</td>\n",
       "      <td>0.001</td>\n",
       "      <td>37.12%</td>\n",
       "      <td>15.96%</td>\n",
       "      <td>2.5716</td>\n",
       "      <td>2.5733</td>\n",
       "      <td>22.5377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35.80%</td>\n",
       "      <td>2.2013</td>\n",
       "      <td>14.72%</td>\n",
       "      <td>3.3726</td>\n",
       "      <td>22.2374</td>\n",
       "      <td>0.001</td>\n",
       "      <td>39.85%</td>\n",
       "      <td>16.99%</td>\n",
       "      <td>2.5539</td>\n",
       "      <td>2.5547</td>\n",
       "      <td>22.2618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>37.77%</td>\n",
       "      <td>2.1136</td>\n",
       "      <td>15.62%</td>\n",
       "      <td>3.3335</td>\n",
       "      <td>22.0326</td>\n",
       "      <td>0.001</td>\n",
       "      <td>40.91%</td>\n",
       "      <td>17.60%</td>\n",
       "      <td>2.5389</td>\n",
       "      <td>2.5701</td>\n",
       "      <td>22.2308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>39.29%</td>\n",
       "      <td>2.0448</td>\n",
       "      <td>16.43%</td>\n",
       "      <td>3.3006</td>\n",
       "      <td>21.8734</td>\n",
       "      <td>0.001</td>\n",
       "      <td>42.18%</td>\n",
       "      <td>18.11%</td>\n",
       "      <td>2.5257</td>\n",
       "      <td>2.5755</td>\n",
       "      <td>22.1097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>40.40%</td>\n",
       "      <td>1.9911</td>\n",
       "      <td>17.04%</td>\n",
       "      <td>3.2706</td>\n",
       "      <td>21.7352</td>\n",
       "      <td>0.001</td>\n",
       "      <td>42.40%</td>\n",
       "      <td>19.15%</td>\n",
       "      <td>2.5134</td>\n",
       "      <td>2.5625</td>\n",
       "      <td>22.0421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>41.25%</td>\n",
       "      <td>1.9452</td>\n",
       "      <td>17.44%</td>\n",
       "      <td>3.2466</td>\n",
       "      <td>21.6345</td>\n",
       "      <td>0.001</td>\n",
       "      <td>43.18%</td>\n",
       "      <td>19.46%</td>\n",
       "      <td>2.5066</td>\n",
       "      <td>2.5185</td>\n",
       "      <td>21.7748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>42.07%</td>\n",
       "      <td>1.9086</td>\n",
       "      <td>17.87%</td>\n",
       "      <td>3.2254</td>\n",
       "      <td>21.5417</td>\n",
       "      <td>0.001</td>\n",
       "      <td>43.95%</td>\n",
       "      <td>19.84%</td>\n",
       "      <td>2.4986</td>\n",
       "      <td>2.5368</td>\n",
       "      <td>21.7821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch From Square Accuracy (Train)  From Square Loss (Train)  \\\n",
       "0      1                        9.97%                    3.4190   \n",
       "1      2                       21.25%                    2.8075   \n",
       "2      3                       28.48%                    2.4963   \n",
       "3      4                       32.86%                    2.3187   \n",
       "4      5                       35.80%                    2.2013   \n",
       "5      6                       37.77%                    2.1136   \n",
       "6      7                       39.29%                    2.0448   \n",
       "7      8                       40.40%                    1.9911   \n",
       "8      9                       41.25%                    1.9452   \n",
       "9     10                       42.07%                    1.9086   \n",
       "\n",
       "  To Square Accuracy (Train)  To Square Loss (Train)  Loss (Train)  \\\n",
       "0                      6.06%                  3.7722       24.8948   \n",
       "1                      9.42%                  3.5953       23.4791   \n",
       "2                     12.12%                  3.4862       22.8679   \n",
       "3                     13.74%                  3.4173       22.4888   \n",
       "4                     14.72%                  3.3726       22.2374   \n",
       "5                     15.62%                  3.3335       22.0326   \n",
       "6                     16.43%                  3.3006       21.8734   \n",
       "7                     17.04%                  3.2706       21.7352   \n",
       "8                     17.44%                  3.2466       21.6345   \n",
       "9                     17.87%                  3.2254       21.5417   \n",
       "\n",
       "   Learning Rate From Square Accuracy (Val) To Square Accuracy (Val)  \\\n",
       "0          0.001                     19.21%                    8.53%   \n",
       "1          0.001                     28.09%                   11.87%   \n",
       "2          0.001                     34.21%                   14.14%   \n",
       "3          0.001                     37.12%                   15.96%   \n",
       "4          0.001                     39.85%                   16.99%   \n",
       "5          0.001                     40.91%                   17.60%   \n",
       "6          0.001                     42.18%                   18.11%   \n",
       "7          0.001                     42.40%                   19.15%   \n",
       "8          0.001                     43.18%                   19.46%   \n",
       "9          0.001                     43.95%                   19.84%   \n",
       "\n",
       "   Position Eval MAE (Train)  Position Eval MAE (Val)  Loss (Val)  \n",
       "0                     2.7405                   2.6667     24.1215  \n",
       "1                     2.6356                   2.7472     24.1544  \n",
       "2                     2.5981                   2.6242     22.9873  \n",
       "3                     2.5716                   2.5733     22.5377  \n",
       "4                     2.5539                   2.5547     22.2618  \n",
       "5                     2.5389                   2.5701     22.2308  \n",
       "6                     2.5257                   2.5755     22.1097  \n",
       "7                     2.5134                   2.5625     22.0421  \n",
       "8                     2.5066                   2.5185     21.7748  \n",
       "9                     2.4986                   2.5368     21.7821  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df5_1 = pd.read_csv('Resultados/training_results_modelo5_1.csv')\n",
    "df5_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>From Square Accuracy (Train)</th>\n",
       "      <th>Loss (Train)</th>\n",
       "      <th>Position Eval MAE (Train)</th>\n",
       "      <th>To Square Accuracy (Train)</th>\n",
       "      <th>From Square Accuracy (Val)</th>\n",
       "      <th>Loss (Val)</th>\n",
       "      <th>To Square Accuracy (Val)</th>\n",
       "      <th>Position Eval MAE (Val)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1122</td>\n",
       "      <td>24.8891</td>\n",
       "      <td>2.7464</td>\n",
       "      <td>0.0688</td>\n",
       "      <td>0.2241</td>\n",
       "      <td>23.7982</td>\n",
       "      <td>0.0999</td>\n",
       "      <td>2.6497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.2440</td>\n",
       "      <td>23.3988</td>\n",
       "      <td>2.6348</td>\n",
       "      <td>0.1102</td>\n",
       "      <td>0.3489</td>\n",
       "      <td>22.8640</td>\n",
       "      <td>0.1419</td>\n",
       "      <td>2.5928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.3201</td>\n",
       "      <td>22.7934</td>\n",
       "      <td>2.5992</td>\n",
       "      <td>0.1386</td>\n",
       "      <td>0.3915</td>\n",
       "      <td>22.5084</td>\n",
       "      <td>0.1609</td>\n",
       "      <td>2.5711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.3548</td>\n",
       "      <td>22.4572</td>\n",
       "      <td>2.5744</td>\n",
       "      <td>0.1518</td>\n",
       "      <td>0.4174</td>\n",
       "      <td>22.2336</td>\n",
       "      <td>0.1757</td>\n",
       "      <td>2.5550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.3753</td>\n",
       "      <td>22.2426</td>\n",
       "      <td>2.5578</td>\n",
       "      <td>0.1612</td>\n",
       "      <td>0.4308</td>\n",
       "      <td>22.0098</td>\n",
       "      <td>0.1798</td>\n",
       "      <td>2.5299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.3888</td>\n",
       "      <td>22.0874</td>\n",
       "      <td>2.5461</td>\n",
       "      <td>0.1684</td>\n",
       "      <td>0.4399</td>\n",
       "      <td>21.9262</td>\n",
       "      <td>0.1892</td>\n",
       "      <td>2.5273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.4001</td>\n",
       "      <td>21.9594</td>\n",
       "      <td>2.5344</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.4485</td>\n",
       "      <td>21.8461</td>\n",
       "      <td>0.1936</td>\n",
       "      <td>2.5273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.4079</td>\n",
       "      <td>21.8624</td>\n",
       "      <td>2.5269</td>\n",
       "      <td>0.1788</td>\n",
       "      <td>0.4581</td>\n",
       "      <td>21.7603</td>\n",
       "      <td>0.1992</td>\n",
       "      <td>2.5145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.4145</td>\n",
       "      <td>21.7742</td>\n",
       "      <td>2.5201</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>0.4557</td>\n",
       "      <td>21.7291</td>\n",
       "      <td>0.2053</td>\n",
       "      <td>2.5194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.4194</td>\n",
       "      <td>21.6988</td>\n",
       "      <td>2.5131</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.4611</td>\n",
       "      <td>21.7160</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>2.5248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Epoch  From Square Accuracy (Train)  Loss (Train)  \\\n",
       "0      1                        0.1122       24.8891   \n",
       "1      2                        0.2440       23.3988   \n",
       "2      3                        0.3201       22.7934   \n",
       "3      4                        0.3548       22.4572   \n",
       "4      5                        0.3753       22.2426   \n",
       "5      6                        0.3888       22.0874   \n",
       "6      7                        0.4001       21.9594   \n",
       "7      8                        0.4079       21.8624   \n",
       "8      9                        0.4145       21.7742   \n",
       "9     10                        0.4194       21.6988   \n",
       "\n",
       "   Position Eval MAE (Train)  To Square Accuracy (Train)  \\\n",
       "0                     2.7464                      0.0688   \n",
       "1                     2.6348                      0.1102   \n",
       "2                     2.5992                      0.1386   \n",
       "3                     2.5744                      0.1518   \n",
       "4                     2.5578                      0.1612   \n",
       "5                     2.5461                      0.1684   \n",
       "6                     2.5344                      0.1739   \n",
       "7                     2.5269                      0.1788   \n",
       "8                     2.5201                      0.1833   \n",
       "9                     2.5131                      0.1880   \n",
       "\n",
       "   From Square Accuracy (Val)  Loss (Val)  To Square Accuracy (Val)  \\\n",
       "0                      0.2241     23.7982                    0.0999   \n",
       "1                      0.3489     22.8640                    0.1419   \n",
       "2                      0.3915     22.5084                    0.1609   \n",
       "3                      0.4174     22.2336                    0.1757   \n",
       "4                      0.4308     22.0098                    0.1798   \n",
       "5                      0.4399     21.9262                    0.1892   \n",
       "6                      0.4485     21.8461                    0.1936   \n",
       "7                      0.4581     21.7603                    0.1992   \n",
       "8                      0.4557     21.7291                    0.2053   \n",
       "9                      0.4611     21.7160                    0.2083   \n",
       "\n",
       "   Position Eval MAE (Val)  \n",
       "0                   2.6497  \n",
       "1                   2.5928  \n",
       "2                   2.5711  \n",
       "3                   2.5550  \n",
       "4                   2.5299  \n",
       "5                   2.5273  \n",
       "6                   2.5273  \n",
       "7                   2.5145  \n",
       "8                   2.5194  \n",
       "9                   2.5248  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5_2 = pd.read_csv('Resultados/training_results_modelo5_2.csv')\n",
    "df5_2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sexta arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cambio de cabezas\n",
    "### unificación de las salidas de from y to \n",
    "### Cambio de la heurística manual por el resultado de la partida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\" X_train, y_train_policy, y_train_result = build_dataset(train_dataset)\n",
    "X_test, y_test_policy, y_test_result = build_dataset(test_dataset)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(8, 8, 17))\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(inputs)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "def res_block(x, filters):\n",
    "    shortcut = x\n",
    "    y = tf.keras.layers.Conv2D(filters, (3, 3), activation='relu', padding='same')(x)\n",
    "    y = tf.keras.layers.BatchNormalization()(y)\n",
    "    y = tf.keras.layers.Conv2D(filters, (3, 3), activation=None, padding='same')(y)\n",
    "    y = tf.keras.layers.BatchNormalization()(y)\n",
    "    y = tf.keras.layers.Add()([shortcut, y])\n",
    "    y = tf.keras.layers.Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "x = res_block(x, 128)\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "x = tf.keras.layers.Dropout(0.4)(x)\n",
    "\n",
    "output_policy = tf.keras.layers.Dense(len(MOVE_INDEX), activation='softmax', name=\"policy_head\")(x)\n",
    "output_value = tf.keras.layers.Dense(1, activation='tanh', name=\"value_head\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=[output_policy, output_value])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0004),\n",
    "    loss={\"policy_head\": \"sparse_categorical_crossentropy\", \"value_head\": \"mse\"},\n",
    "    metrics={\"policy_head\": \"accuracy\", \"value_head\": \"mae\"}\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    {\"policy_head\": y_train_policy, \"value_head\": y_train_result},\n",
    "    validation_data=(X_test, {\"policy_head\": y_test_policy, \"value_head\": y_test_result}),\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    verbose=2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.0001)\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.save(\"modelo_fase_1B.h5\")\n",
    "print(\"Modelo reentrenado con valor de partida y guardado.\")\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
